---
title: "Graphing CMU Varsity Swimming and Diving"
output:
  html_document:
    fig_width: 12
---

# Introduction

This dataset was scraped from [collegeswimming.com](collegeswimming.com), which is widely used in the swimming community to track swims from around the NCAA, NAIA, and NJCAA. This site tracks *every* swim done at almost every collegiate meet, which allows us an incredibly granular look at collegiate swimming performance in the US. Each row of the dataset looks like the following:

`
  233047|100 Y Fly|2014-12-06|49.96|2015|684
`

This particular row was the 100 Yard Fly swam on December 6th, 2014 in a time of 49.96 during the 2014-2015 season. The 684 on the end is the "power points" awarded to the swim. "Power Points" are a standard method used by FINA, the international governing body of swimming, to standardize each event. You can see the full justification [here](http://archives.fina.org/H2O/docs/FINApoints/FINA_Points_Table_20150205.pdf), but the basic formula as follows:

$$
  \text{Power Points} = 1000 \times  (\frac{\text{Record Time}}{\text{Actual Time}}) ^ 3
$$
This is a standard way of comparing swims across multiple events. For example, this allows us to compare men to women, sprinters to distance swimmers, or if you really wanted to get crazy, backstrokers to breaststrokers. Very simply, faster times garner larger scores, with a record-tying performance getting awarded exactly 1000 points.

This allows us to analytically approach questions which could in the past only be ballparked. What was the best swim of the season? How effective are taper cycles? What are a team's best events? In the aggregate, we can compare entire teams, events, and even seasons in a relatively objective way.

In this analysis, we are only going to look at what is happening during the *collegiate* swimming season, which typically runs September to March. While collegeswimming.com tracks swims the whole year, that swimming is largely dependent on organizations which operate outside of the NCAA. While it might be helpful from a prediction point of view (how might a summer meet predict a winter meet's performance?), it's simply not relevant in the context of visualization.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(lubridate)
swims <- read.csv("../data/swims.csv", sep = "|", header=TRUE, row.names = NULL)

# make each of the overlap by moving them to this year
swims$date <- as.Date(swims$date)
swims$date <- ymd(swims$date) + years(2017 - swims$season)
swims$season <- as.numeric(swims$season)

# exclude seasons with fewer than 50 swims (usually user-inputting times which aren't relevant)
sufficient.seasons <- names(which(table(swims$season) > 100))
swims <- swims[which(swims$season %in% sufficient.seasons),]

inline_hook <- function(x) {
  if (is.numeric(x)) {
    format(x, digits = 2)
  } else x
}
knitr::knit_hooks$set(inline = inline_hook)
```

# Visualizing the Data

Let's start with just every single swim, in power points.

```{r, warning = FALSE, fig.height = 4, echo = FALSE, message = FALSE, cache = TRUE}
library(ggplot2)

ggplot(swims) +
  aes(x = date, y = points) +
  geom_point() +
  scale_y_continuous(limits = c(300, 820))
```

Now let's add a smoothing spline to get a very general feel of the data. Here we can see the natural training/taper cycle with the Kenyon invite in December, UAAs in February, and NCAAs in March.

```{r, warning = FALSE, echo = FALSE, message = FALSE, cache = TRUE}
ggplot(swims) +
  aes(x = date, y = points) +
  geom_point(colour = "grey") +
  geom_smooth(colour = "black", se = FALSE) +
  scale_y_continuous(limits = c(300, 820))
```

We can also look at each season separately, seeing if we've improved over the years:

```{r, warning = FALSE, echo = FALSE, message = FALSE, cache = TRUE}
ggplot(swims) +
  aes(x = date, y = points) +
  geom_point(colour = "grey") +
  geom_smooth(aes(group = season, colour = season), se = FALSE) +
  scale_y_continuous(limits = c(300, 820)) +
  scale_colour_gradient(limits = c(min(swims$season), max(swims$season)), low = "red", high = "green")
```

Here, when we depict more recent seasons as greener, while older seasons are more red. Since all of the "red" seasons are below the "green" seasons, we see we generally do better as time goes on.

```{r, cache = TRUE, echo = FALSE}
generate.model.bounds <- function(id, data = swims) {
  persons.swims <- na.omit( data[data$swimmerID == id,] )
  if(nrow(persons.swims) < 10) { return(0) } # they didn't swim enough to merit a "taper"
  individual.model <- smooth.spline(persons.swims$date, persons.swims$points)
  y.hat <- predict(individual.model)$y
  return(max(y.hat) - min(y.hat))
}

ids <- as.numeric(rownames(table(swims$swimmerID)))
points <- sapply(ids, generate.model.bounds)
taper.queen <- ids[which(points == max(points))]
```

On the topic of individual performance, we can also look at the biggest "taper queen" on the team, or simply whoever has the largest difference between in-season and taper performance. Through this analysis, you can visit the profile of the quantifiably biggest taper queen ever [here](https://www.collegeswimming.com/swimmer/`r taper.queen`/), and we can visualize their career below:

```{r, warning = FALSE, echo = FALSE, message = FALSE, cache = TRUE}
swims$is.taper.queen <- ifelse(swims$swimmerID == taper.queen, "yes", "no")

ggplot(swims) +
  aes(x = date, y = points) +
  geom_point(colour = "grey") +
  geom_smooth(aes(colour = is.taper.queen), se = FALSE) +
  scale_y_continuous(limits = c(300, 820)) +
  scale_colour_manual(values = c("black","red"))
```

As we can see, the swings associated with this swimmer *much* more pronounced, having a much larger difference between their in-season and tapered swimming.

Another method of analysis is to try and guess see what a team's records *should* look. For the approach, we can take the date at which a team gets its maximum power points, and use a `predict.time` above at the optimal date to predict a time for every event. Here we ca 

```{r, warning = FALSE, echo = FALSE}
library(jsonlite)
library(knitr)

predict.time <- function(model, date, record) {
  points <- predict(model, as.numeric(as.Date(date)))$y
  predicted.time <- ((points / 1000) ** (-1/3)) * record
  return(predicted.time)
}

swims.na.omit <- na.omit(swims)
model.taper <- smooth.spline(swims.na.omit$date, swims.na.omit$points)
records <- read_json("../scraping/base-times-men.json")

y.hat.model.taper <- predict(model.taper)

max.performance <- max(y.hat.model.taper$y)
best.date <- y.hat.model.taper$x[which(y.hat.model.taper$y == max.performance)]
best.date <- as.Date(best.date, origin = "1970-01-01")

get.time.for.record <- function(record) {
  record.time <- as.double(records[record])
  if(is.na(record.time)) {
    record.time <- period_to_seconds(ms(records[record]))
  }
  
  y.hat.time <- predict.time(model.taper, best.date, record.time)
  return(round(y.hat.time, 2))
}

format.time <- function(time) {
  td <- seconds_to_period(time)
  sprintf('%02d:%0.2f', minute(td), second(td))
}

sapply(sapply(names(records), get.time.for.record), format.time)
```

